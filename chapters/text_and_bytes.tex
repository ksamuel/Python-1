\chapter{All the things you wish you didn't have to know about I/O, text and bytes}\label{chap:text_and_bytes}

It's time we have \textquote{The Talk}.

The least obvious part about moving from Python 2 to 3 is dealing with inputs and outputs, the bytes in between and how people manipulate text.

In fact, I give \textquote{The Talk} even to people that are not migrating anything, because my experience is that most coders have no idea what those \lstinline{UnicodeDecodeError} are all about.

So read it with attention, it will make all Python coding, present and future, way clearer. To be honest, it will make all coding clearer, in any language: encoding is an universal problem.

First, we'll start with a deep explanation on how things work. We'll only focus on the differences between Python 2 and 3 after that.

It's a pretty long disgression, but it will pay off.

\section{A tale of text and bytes}

\subsection{Bytes are not text}

The whole story must start with the \lstinline{bytes()} type, or rather, what's in it.

Thirty years ago you didn't have to lecture your audience about that: it was mostly low level programmers, they new all about bare metal data representation. If you are one, you can skip this section.

But nowaday, a lot of Python coders have only used high level languages, with types abstracting the details of codecs, mantis or endians.

So what's in a \lstinline{bytes()} ?

Well, in Python, \lstinline{bytes()} is used to hold an arbitrary sequence of numbers, each number being a byte, meaning an integer between 0 and 255, in binary.

You see, in computing there is no such things as text, float, or jpeg. We got sequences of 0 and 1. That's all.

For convenience, we group them by packets of 8, which in binary goes from 00000000 (0 in base 10) to 11111111 (256 in base 10).

Then we make up conventions.

Everything in computing is just made of conventions. Arbitrary decisions, driven by technical needs, limits or human factors, that people agree on, stating that this combinaison of numbers mean something. They don't have any meaning by themself mind you, but we have specs somewhere saying they do. And so we can use them as such.

For example, let's take this sequence of 6 bytes: 10000000, 11, 1011101, 1110001, 0, 101110. In base 10, it's 128, 3, 93, 113, 0, 46.

This sequence of numbers means nothing by itself.

But some people decided that it could be used to represent C types, more specifically 3 short integers: 896, 29021, 11776.

Easy to see in Python:

\begin{py2and3}
>>> d = bytes([0b10000000, 0b11, 0b1011101, 0b1110001, 0b0, 0b101110])
>>> list(d)
[128, 3, 93, 113, 0, 46]
>>> import struct
>>> struct.unpack('hhh', d)
(896, 29021, 11776)
\end{py2and3}

However, some other people, authors of the pickle protocol, decided that the very same bytes could mean an empty list:

\begin{py2and3}
>>> d = bytes([0b10000000, 0b11, 0b1011101, 0b1110001, 0b0, 0b101110])
>>> import pickle
>>> pickle.loads(d)
[]
\end{py2and3}

Another example ?

The numbers 1100001, 1100010, 1100011, 1100100 (or 97, 98, 99, 100 in base 10) can be interpretted as text if you read them using the ASCII convention:

\begin{py2and3}
>>> data = bytes([0b1100001, 0b1100010, 0b1100011, 0b1100100])
>>> print(data.decode('ascii'))
abcd
\end{py2and3}

But the exact same bytes represent the number 1633837924 if you read them as an unsigned big endian integer:

\begin{py2and3}
>>> data = bytes([0b1100001, 0b1100010, 0b1100011, 0b1100100])
>>> import struct
>>> struct.unpack('>I', data)
(1633837924,)
\end{py2and3}

Bottom line, you cannot understand what's in a \lstinline{bytes()} type unless you know what format it is in. It can hold anything.

Other data types are just higher level of abstractions that hide the complexity of this reality. Handling raw bytes for doing stats or parsing a file wouldn't be very fun.

But a big problem can easily arise: many coders think that the \lstinline{bytes()} type contains text.

It's not the case, bytes can represent anything, including of course, text.

However, many old languages use arrays of bytes to manipulate characters (hence the name 'strings'). Python 2 does,and it caused a lot of confusion. Also, when you types \lstinline{bytes()} in the terminal, any Python version will  show you a text representation:


\begin{py2and3}
>>> bytes([0b10000000, 0b11, 0b1011101, 0b1110001, 0b0, 0b101110])
b'\x80\x03]q\x00.'
>>> bytes([0b1100001, 0b1100010, 0b1100011, 0b1100100])
b'abcd'
\end{py2and3}

In fact, you can even create \lstinline{bytes()} using a text notation:

\begin{py2and3}
>>> list(b"Those are numbers")
[84, 104, 111, 115, 101, 32, 97, 114, 101, 32, 110, 117, 109, 98, 101, 114, 115]
\end{py2and3}

This is for convenience. If you manipulates bytes a lot, it gets easier to recognise values from their ASCII letter representations or their hexadecimal values. So this is what Python gives you by default.

It doesn't mean \lstinline{bytes()} hold text though. It can. But even if it doesn't, Python will try to show you a human readable representation of any mess you throw at it.

\subsection{str() for manipulating text}

In, Python, well, in Python 3 at least, the type \lstinline{str()} is the high level data structure tailored to manipulate text. Not \lstinline{bytes()}.

If you are sure some \lstinline{bytes()} contains text. And if you know what character set the bytes has been written in (like utf-8, latin-1, IS0-8859, etc.), you can convert your \lstinline{bytes()} to a \lstinline{str()} by decoding it:

\begin{py3}
>>> b'\xe8\x9f\x92\xe8\x9b\x87'.decode('utf8')
'蟒蛇'
\end{py3}

Yes, you should do this even if it's ASCII:

\begin{py3}
>>> type(b'abcd')
<class 'bytes'>
>>> type(b'abcd'.decode('ascii'))
<class 'str'>
\end{py3}

The opposite is true. You can turn \lstinline{str()} to \lstinline{bytes()} by encoding:

\begin{py3}
>>> type("éèêë")
<class 'str'>
>>> "éèêë".encode('utf8')
b'\xc3\xa9\xc3\xa8\xc3\xaa\xc3\xab'
\end{py3}

Those bytes can then be put in a file, or sent through the network. Indeed, the rest of the world cannot manipulate Python types, they understand only bytes. Of course, then need to know the convention you used to understand what you meant.

For text, there are many, many conventions, containing the characters sets (the table stating which bytes combinations matches which graphem, often a character) and a spec of rules to apply when assembling the content. We call those \textquote{encodings}.

A quick look at the encodings supported by Python gives us more than three hundred of them:

\begin{py3}
>>> import encodings
>>> list(encodings.aliases.aliases.keys())[:10]
['chinese', 'ksx1001', 'ibm039', '865', 'iso_8859_9', 'iso_2022_jp_ext', 'iso_2022_jp_1', 'u32', 'l10', 'csiso2022jp']
>>> len(encodings.aliases.aliases.keys())
322
\end{py3}

Among all of those, the most popular standards are ASCII (\lstinline|'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!"#$%&\'()*+,-./:;<=>?@[\\]^_`{\|\}~|), ISO-8859 (a.k.a latin-1, used for extra west european characters), cp850 (default encoding of cmd.exe on Windows) and unicode.

Unicode is a very successful attempt at creating something you can use for most text, including European languages, arab, russian, asian languages, and even some dead languages.

The most popular implementation of unicode is UTF-8. It won because any ASCII is valid UTF-8, which helped the transition.

Basically, unless you have a very good reason and you know what you're doing, you should be using UTF-8 everywhere.

\section{Good practice of text manipulation}

For a painless text experience, in Python, but also in other programming languages, you should follow a few rules.

\subsection{Rule 1: your code files should be in UTF-8}

Any text is a bunch of bytes organised according to an encoding. You code files are no exception. Any decent code editor allow you to select the encoding. Choose UTF-8.

\subsection{Rule 2: when you get an input, .decode()}

Any data you manipulate in your program that wasn't hard coded in your file is an input. User filling up a form, loading in a file, getting a network packet, reading a database reasponse, prompting from the terminal. All those are data comming from the outside world inside your program.

They all come as bytes, and some of those bytes may be text. If you are sure they are text and know the encoding, call \lstinline{.decode()} to get a \lstinline{str()}.

Sometimes, wrappers will make it easier for you. E.G, you could do:

\begin{py3}
>>> with open(some_file, 'rb') as f:
...     for line in f:
...         # line is a bytes(), we
...         # need to decode
...         print(line.decode('utf8'))
Moo.
I said, moo.
Look I'm just a cow, ok?
\end{py3}

But \lstinline{open()} can do it for you:

\begin{py3}
>>> with open(some_file, 'r', encoding="utf8") as f:
...     for line in f:
...         print(line) # line is a string
Moo.
I said, moo.
Look I'm just a cow, ok?
\end{py3}

Some things are even easier. Reading from the terminal is automatically decoded by Python. You have nothing to do. Some 3rd party libraries, like \lstinline{requests} try to find out the encoding received from the network, and decode it transparently for you:

\begin{py3}
>>> import requests
>>> response = requests.get('http://python.org')
>>> response.encoding
'utf-8'
>>> type(response.text)
<class 'str'>
\end{py3}

But if you receive \lstinline{bytes()}, you should decode it manually.

\subsection{Rule 3: when you do an output, .encode()}

At some point you will need to get data out of your program, otherwise it wouldn't be very useful. If this data is text, you should encode it before any output.

Everything that comes out of your program means there is an output: printing on the terminal, making a query to a database, sending packets through a socket, writting a file, etc.

And if the data you is in a \lstinline{str()}, you must turn it into a \lstinline{bytes()} using \lstinline{.encode()}.

As for reading, sometime it will be made easy, like with \lstinline{open()}. Libs may do it for you. Or sometime you will have to do it manually.

Of course, unless you have a very good reason (client requires it, old specs, legacy system, embdeded device with limited resources...), always use UTF-8.

\section{But how do I know the encoding being used ?}

Now that it is clear that \lstinline{bytes()} contains arbitrary bytes, that those bytes may be text, and that to use it, you need the convention that produced it, how to find it?

Sometimes, there is an API for it. E.G, standard I/O and file system encodings are exposed by Python:

\begin{py2and3}
>>> import sys
>>> sys.stdout.encoding
'UTF-8'
>>> sys.stdin.encoding
'UTF-8'
>>> sys.stderr.encoding
'UTF-8'
>>> sys.getfilesystemencoding()
'UTF-8'
\end{py2and3}

You probably won't need those in particular, since Python transparently decode/encode FS calls and terminal printing/reading. But you get the picture: some libraries will let you know the encoding.

Sometimes, you can ask a system the information. E.G: database let you send an SQL query to get the encoding used for tables.

Documents and protocols may also include the information in metadata. HTML and XML often have encodings stated in the first lines, HTTP headers may contain the encoding too.

Specs and documentation also can very well define the mandatory encoding. JSON is a famous example, as it must be UTF-8 to fit the standard.

Finally, you may just ask the developper/user that made or used the software that produced the text.

Unfortunaly, it is often not enough. Many developpers don't know anything about encodings, and don't know what they used, or that they used one at all. Sometime, they used or declared the wrong one. Sometime the author is missing, the doc obsolete, and the information is nowhere to be found.

So is there a sure way to find out the encoding ?

No.

Texts don't have to carry with them their encodings. A lot of files are just lying there, without any way to know for sure. Fun fact, the filename encoding (which depends of the file system used) can very well be different than the file content encoding (which depends of what the user chose).

However, many softwares such as code editors or web browsers have been trying to solve this problem for years, and developped pretty decent heuristics to attempt detecting text formats.

One of those library is \href{https://pypi.org/project/chardet/}{Chardet}, a FOSS project that originated from Mozilla, and ported to Python. Luckily, it's compatible with Python 2 and 3!

After a \lstinline{pip install chardet}, you can do:

\begin{py2and3}
>>> import chardet
>>> chardet.detect("Ça va trancher chérie !".encode('utf8'))
{'confidence': 0.7525, 'language': '', 'encoding': 'utf-8'}
>>> chardet.detect("Ça va trancher chérie !".encode('ISO-8859-1'))
{'confidence': 0.73, 'language': '', 'encoding': 'ISO-8859-1'}
\end{py2and3}

Of course, this is not exact science, but probabilities, and sometimes it's wrong. But it's pretty good, and it also has a command line version to be used on an entire file.

If none of that works, it's an acceptable strategy to try popular encodings one by one:

\begin{itemize}
\item UTF8
\item ISO-8859-1
\item Windows-1252
\item CP850
\item ASCII
\end{itemize}

This works espacially well in environnements with latin alphabets. Also, if you are doing it dynamically, a \lstinline{try}/\lstinline{except} on \lstinline{UnicodeEncodeError} or \lstinline{UnicodeDecodeError} will be required.

\section{Workarounds for when things go wrong}

Despite your best efforts, something will fail. The world is imperfect. Double encodings, bugs, badly declared metadata, old systems and clients with limited knowledge are part of the drill.

In that case what can you do ?

If you are getting an input, you must \lstinline{.decode()}. Find out the encoding from metadata/specs/query/API. Try it. If that doesn't work, try Chardet. If that fails, try the list of encodings I gave you. And if that still doesn't give you what you need, use UTF-8 with an error handler.

Without any error handling strategy:

\begin{py3}
>>> encoded_text = "Ça va trancher chérie !".encode('cp850')
>>> encoded_text.decode('utf8')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/user/.local/share/virtualenvs/book/lib/python2.7/encodings/utf_8.py", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte
\end{py3}

With one:

\begin{py3}
>>> encoded_text = "Ça va trancher chérie !".encode('cp850')
>>> print(encoded_text.decode('utf8', errors="replace"))
�a va trancher ch�rie !
>>> print(encoded_text.decode('utf8', errors="ignore"))
a va trancher chrie !
\end{py3}

This works in Python 2 as well, but needs \lstinline{unicode()}, which will talk about later.

If you have to share text with the rest of the world, you have the opposite problem. You must \lstinline{.encode()}. You can always encode to UTF8, you won't get an error from that. A problem can arise, however, when some systems that consumes the resulting bytes are not be able to, because they don't support UTF-8. The Windows terminal used CP-850 by default for a long time. I worked with systems from the US Health administration using SOAP, and supporting only ASCII.

So again, try to find out what encodings does the system support, and target that. If you can't find it, try the list of encodings I gave you, and feed it to the system to see what works. In doubt, use UTF-8. In last resort, use ASCII.

However, you may hold some text containing characters that cannot be represented in the target encoding. Imagine an blog article containing the sentence \textquote{The Chinese title of "Crouching Tiger, Hidden Dragon" is 臥虎藏龍}.

It would be easy to encode in UTF8:

\begin{py3}
>>> 'The Chinese title of "Crouching Tiger, Hidden Dragon" is 臥虎藏龍'.encode('utf8')
'The Chinese title of "Crouching Tiger, Hidden Dragon" is \xe8\x87\xa5\xe8\x99\x8e\xe8\x97\x8f\xe9\xbe\x8d'
\end{py3}

But it would fail in Windows-1252 since it doesn't allow ideogram:

\begin{py3}
>>> 'The Chinese title of "Crouching Tiger, Hidden Dragon" is 臥虎藏龍'.encode('windows-1252')
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "/home/user/.local/share/virtualenvs/book/lib/python2.7/encodings/cp1252.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_table)
UnicodeEncodeError: 'charmap' codec can't encode characters in position 57-60: character maps to <undefined>
\end{py3}

If the target system only understands an limited encoding, you will have to try to convert your text to something it can understand. You will lose informations in the process, but it's a trade off you'll need to make.

For latin alphabets, you can just do:

\begin{py3}
>>> import unicodedata
>>> neruda = "Podrán cortar todas las flores, pero no podrán detener la primavera"
>>> unicodedata.normalize('NFKD', neruda).encode('ascii','ignore')
b'Podran cortar todas las flores, pero no podran detener la primavera'
\end{py3}

But it won't work for East Europe, Arab or Asian languages.

For those, you can use the \href{https://pypi.org/project/Unidecode/}{Unidecode} library, which is compatible Python 2 and 3.

After a \lstinline{pip install unidecode}, do:

\begin{py3}
>>> import unidecode
>>> unidecode.unidecode('The Chinese title of "Crouching Tiger, Hidden Dragon" is 臥虎藏龍')
'The Chinese title of "Crouching Tiger, Hidden Dragon" is Wo Hu Cang Long '
\end{py3}

Please note it will most probably result in nonsensical text. And sometimes pretty funny, or even offensive ones. The goal here is to get something you can share wit very limited systems, with the most acceptable degradation possible. There is no silver bullet.

\section{The problem with Python 2}

Now that you know how things work, let's see why this is was hard in Python 2.

In Python 3, you have \lstinline{bytes()} to hold a sequence of any type of byte arrangements, and \lstinline{str()} for manipulating text.

However, in Python 2, \lstinline{str()} is actually very similar to \lstinline{bytes()}. To fix this, \lstinline{unicode()} has been introduced, that behaves more like Python 3 \lstinline{str()}. This means:

\begin{table}[h]
\begin{tabular}{cccll}
\cline{1-3}
\multicolumn{1}{|c|}{\textit{\textbf{Python Version}}} & \multicolumn{1}{c|}{\textit{\textbf{Type for bytes}}} & \multicolumn{1}{c|}{\textit{\textbf{Type for text}}} &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{2}                                & \multicolumn{1}{c|}{str()}                            & \multicolumn{1}{c|}{unicode()}                       &  &  \\ \cline{1-3}
\multicolumn{1}{|c|}{3}                                & \multicolumn{1}{c|}{bytes()}                          & \multicolumn{1}{c|}{str()}                           &  &  \\ \cline{1-3}
\multicolumn{1}{l}{}                                   & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                 &  &
\end{tabular}
\end{table}

Very confusing indeed. What's even more confusing is that, because of a long time Python 2 didn't have \lstinline{unicode()}, strings manipulations were done on \lstinline{str()} (a.k.a \lstinline{bytes()} in Python 2). In fact, if you just write a string like this:

\begin{py2}
message = "Jörð is the Norse godess of Earth"
\end{py2}

This creates a \lstinline{str()} (so something like a \lstinline{bytes()}), and the encoding it uses depends of the encoding of the code file!

Some other traps:

\begin{itemize}
    \item If you didn't specify the encoding of the file in a magic comment (e.g: \lstinline{# coding: utf8}), Python 2 used ASCII by default and crashed.
    \item You could concatenate, simply using the \lstinline{+} operator, two strings with different encoding. Python 2 wouldn't say a word: it's just a bigger sequence of bytes after all. You just ended up with a corrupted string.
    \item If you tried to concatenate \lstinline{unicode()} with a \lstinline{str()}, Python would try to automatically decode the \lstinline{str()} using ASCII, crashing with a puzzling \lstinline{UnicodeDecodeError}.
\end{itemize}

The way Python 2 manipulated text worked most of the time anyway, because most coders are coding in English, in an ASCII environnement. The pain really started with the success of the Internet, when the entire world starting exchanging a lot of data containing many languages.

\section{Migrating your strings}

Finally !

%# coding: utf-8

%__future__

u

decode

encode

codecs.open() (and perfs)

formatting bytes

always call open() with an encoding or else

buffer. Par exemple, pour écrire des octets sur stdout, utilisez sys.stdout.buffer.write(b'abc').

unfortunately, changing sys.stdout to accept only unicode breaks a lot of libraries that expect it to accept encoded bytestrings. – nosklo Dec 4 '09 at 19:14


\section{Fun with file paths}

\section{Wait, there is I/O}

% http://www.dabeaz.com/python3io_2010/MasteringIO.pdf

Text strings in Python 3 require either 2x as much memory to store as Python 2


bad filenames were easy to create in python 2

If you ever see a \lstinline{\udcxx} character, it means that a non-decodable byte was passed in from a system interface

s.decode('utf-8','surrogateescape')

TextIOWrapper 10 times faster than codecs.open

basestring

%All backslashes in raw string literals are interpreted literally. This means that '\U' and '\u' escapes in raw strings are not treated specially. For example, r'\u20ac' is a string of 6 characters in Python 3.0, whereas in 2.6, ur'\u20ac' was the single “euro” character. (Of course, this change only affects raw string literals; the euro character is '\u20ac' in Python 3.0.)

% has been removed and reinstroduced

pathlib

%# coding: utf8

\section{file()}

\begin{py2}
from io import IOBase

if isinstance(someobj, IOBase):
\end{py2}

\section{from buffer() tp memoryview()}

Those two functions respectively create objects of the same name - a \lstinline{buffer} and a \lstinline{memoryview}. Both are a way to get a subset of something without copying it:

\begin{py2}
>>> donkey_lines = "Are we there yet ?\n" * 10000
>>> # this copies the data into a new object:
>>> for x in donkey_lines[:6]:
...    print(x)
A
r
e

w
e
>>> # those don't:
>>> for x in buffer(donkey_lines, 0, 5):
...    print(x)
A
r
e

w
e
>>> for x in memoryview(donkey_lines)[:5]:
...    print(x)
A
r
e

w
e
\end{py2}

It works on \lstinline{bytes()}, \lstinline{bytearray}, \lstinline{array.array}...Everything that implements the so-called \textquote{buffer protocol}. This is a very nice optimization that can save quite a lot of memory/CPU if you manipulate huge chunks of bytes and pass around subset of them, e.g: to files or sockets.

With the rise of performant c libs wrapped in Python (numpy, GUI toolkits, database drivers, etc.), the need for more information about the underlying data than what \lstinline{buffer()} was offering became important. \lstinline{memoryview} provides an answer to that, being able to return the shape, dimension or type or the object behind it.

\lstinline{buffer()} is not more in Python 3, just replace it with \lstinline{memoryview()}. The later exists in Python 2.7, plus it does the same thing, just better. The only difficulty will be that \lstinline{buffer()} accepts \lstinline{unicode()} objects but \lstinline{memoryview()} only accept bytes, and so you'll need to encode them.

So:

\begin{py2}
from imaginary_lib import get_tps_reports, send_those_bytes

# Imaginary code returning a lot of bytes
tps_reports = get_tps_reports()

# Imaginary code writting those bytes to a sockets by chunks
# of 1Mio
step = 3
stop = len(tps_reports)
for i in range(0, stop, step):
    send_those_bytes(buffer(tps_reports, i, step))

\end{py2}
